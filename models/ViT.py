# -*- coding: utf-8 -*-
"""ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bbwiX7Z8TT6XVGYCkdkqs77CR_kagU95
"""

import torch.nn as nn
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
import numpy as np

# code adapted from https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8/

class Patch_Tokenization(nn.Module):
  def __init__(self,
        img_size=(3,32,32),
        patch_size=16,
        emb_dim=768):
    '''
    img_size: size of the image, (C, H, W)
    patch_size: size of the patch, P
    emb_dim: embedding dimension, D
    '''
    super().__init__()
    self.img_size = img_size
    self.patch_size = patch_size
    self.emb_dim = emb_dim
    C, H, W = self.img_size
    assert H % self.patch_size == 0, 'Image height must be divisible by patch size'
    assert W % self.patch_size == 0, 'Image width must be divisible by patch size'
    self.num_patches = (H * W) / (patch_size**2) # H*W/P^2

    self.patch_split = nn.Unfold(kernel_size=patch_size,stride=patch_size,padding=0) # non-overlaping patches
    self.embedding_layer = nn.Linear(patch_size*patch_size*C, emb_dim) # token_dim x emb_dim

  def forward(self,x):
    # x: (B, C, H, W) can either be the raw image or the output from a CNN
    x = self.patch_split(x) # (B, C*P^2, H*W/P^2) or (batch_size, token_dim, num_patches)
    x = x.transpose(2,1) # (batch_size, num_patches, token_dim)
    x = self.embedding_layer(x) # (batch_size, num_patches, emb_dim)
    return x

class Token_Processing(nn.Module):
  # prepend class token and add positional encoding
  def __init__(self, emb_dim):
    super().__init__()
    self.class_token = nn.Parameter(torch.zeros(1,1,emb_dim)) # learnable class embedding

  def sinusoid_encoding(self,num_tokens,emb_dim):
    '''
    num_tokens: number of tokens in the sequence
    emb_dim: embedding dimension
    '''
    def get_angle_vector(pos):
      return [pos / np.power(10000, 2*(i//2)/emb_dim) for i in range(emb_dim)]

    angle_rads = np.array([get_angle_vector(pos_i) for pos_i in range(num_tokens)]) # a (num_tokens, emb_dim) array
    angle_rads[:,0::2] = np.cos(angle_rads[:,0::2])
    angle_rads[:,1::2] = np.sin(angle_rads[:,1::2])
    return angle_rads

  def forward(self,x):
    # shape of x (batch_size, num_patches, emb_dim)
    batch_size, num_patches, emb_dim = x.shape
    cls = self.class_token.expand(batch_size, -1, -1) # (batch_size, 1, emb_dim)
    x = torch.cat((cls,x),dim=1) # (batch_size, num_patches+1, emb_dim)
    pos_encoding = self.sinusoid_encoding(num_patches+1,emb_dim) # (num_patches+1, emb_dim)
    pos_encoding = torch.from_numpy(pos_encoding).type(x.dtype).to(x.device) # (num_patches+1, emb_dim)
    x = x + pos_encoding # broadcasting to (batch_size, num_patches+1, emb_dim)
    return x

class TransformerEncoderCell(nn.Module):
  def __init__(self,
        emb_dim,
        ff_dim,
        num_heads=4,
        dropout=0,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm):
    '''
    emb_dim: embedding dimension
    num_heads: number of heads in multi-head attention
    ff_dim: hidden dimension in MLP
    dropout: dropout rate in multi-head attention
    act_layer: activation layer in MLP
    norm_layer: normalization layer in multi-head attention
    '''
    super().__init__()

    self.norm1 = norm_layer(emb_dim)
    self.attn = nn.MultiheadAttention(embed_dim=emb_dim,num_heads=num_heads,dropout=dropout,batch_first=True)

    self.MLP = nn.Sequential(
        nn.Linear(emb_dim,ff_dim),
        act_layer(),
        nn.Linear(ff_dim,emb_dim),
    )
    self.norm2 = norm_layer(emb_dim)
    self.dropout = nn.Dropout(dropout)

  def forward(self,x):
    # shape of x (batch_size, num_patches+1, emb_dim)
    x = x + self.attn(self.norm1(x),self.norm1(x),self.norm1(x),need_weights=False)[0]
    x = x + self.dropout(self.MLP(self.norm2(x)))
    return x

class Prediction_Processing(nn.Module):
  def __init__(self, emb_dim, output_dim=1):
    super().__init__()
    self.linear_layer = nn.Linear(emb_dim,output_dim)

  def forward(self,x):
    # shape of x (batch_size, num_patches+1, emb_dim)
    x = self.linear_layer(x[:,0]) # (batch_size, output_dim)
    return x

class ViT(nn.Module):
  def __init__(self,
        img_size,
        patch_size=16,
        depth=12,
        emb_dim=768,
        ff_dim=3072,
        num_heads=4,
        dropout=0,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
        output_dim=1):
    super().__init__()
    self.patch_token_layer = Patch_Tokenization(img_size=img_size,patch_size=patch_size,emb_dim=emb_dim)
    self.token_processing_layer = Token_Processing(emb_dim=emb_dim)
    self.encoder_stack = nn.ModuleList([TransformerEncoderCell(emb_dim=emb_dim,ff_dim=ff_dim,num_heads=num_heads,dropout=dropout,\
                                 act_layer=act_layer,norm_layer=norm_layer) for _ in range(depth)])
    self.norm = norm_layer(emb_dim)
    self.prediction_layer = Prediction_Processing(emb_dim=emb_dim,output_dim=output_dim)
    self.sigmoid = nn.Sigmoid()

  def forward(self,x):
    # shape of x (batch_size, C, H, W)
    x = self.patch_token_layer(x) # (batch_size, num_patches, emb_dim)
    x = self.token_processing_layer(x) # (batch_size, num_patches+1, emb_dim)
    for layer in self.encoder_stack:
      x = layer(x)
    # (batch_size, num_patches+1, emb_dim)
    x = self.norm(x) # (batch_size, num_patches+1, emb_dim)
    x = self.prediction_layer(x) # (batch_size, output_dim)
    x = self.sigmoid(x)
    return x

