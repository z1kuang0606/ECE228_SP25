# -*- coding: utf-8 -*-
"""Augmented_NODE_conv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KESqVOuGLVElO94dYvNwk0JnaR9Af9I
"""

import torch
import torch.nn as nn
from torchdiffeq import odeint, odeint_adjoint

# time-dependent conv layer

class TimeConv(nn.Conv2d):
  def __init__(self,in_channels,*args,**kwargs):
    '''
    append one extra input channel as the time variable
    '''
    super().__init__(in_channels+1,*args,**kwargs)

  def forward(self,t,x):
    '''
    t: time variable, a tensor
    x: input image (b, c, h, w)
    '''
    tt = torch.ones_like(x[:,:1,:,:]) * t # (b, 1, h, w)
    ttx = torch.cat([tt,x],dim=1)
    return super().forward(ttx)

# ODE function that defines the dynamics
class ODEFunc(nn.Module):
  def __init__(self,device='cpu',img_size=(3,32,32),hidden_channels=64,aug_channels=5,num_layers=3,act=nn.ReLU()):
    super().__init__()
    self.device = device
    self.real_in_channels, self.h, self.w = img_size
    self.real_in_channels += aug_channels # augmented channels
    self.hidden_channels = hidden_channels
    self.aug_channels = aug_channels
    self.num_layers = num_layers
    self.act = act
    self.real_out_channels = self.real_in_channels

    self.conv1 = TimeConv(in_channels=self.real_in_channels,out_channels=self.hidden_channels,kernel_size=1,padding=0,stride=1)
    self.non_linearity1 = self.act

    self.seq = nn.Sequential(self.conv1,self.non_linearity1)
    for i in range(num_layers):
      self.seq.append(TimeConv(in_channels=self.hidden_channels,out_channels=self.hidden_channels,kernel_size=3,padding=1,stride=1))
      self.seq.append(self.act)
    self.seq.append(TimeConv(in_channels=self.hidden_channels,out_channels=self.real_out_channels,kernel_size=1,padding=0,stride=1))
    self.seq.append(self.act)

  def forward(self,t,x):
    # dimension of x is (b, c+aug, h, w)
    # t is a tensor
    for layer in self.seq:
      if isinstance(layer,TimeConv):
        x = layer(t,x)
      else:
        x = layer(x)
    return x

# ODE block that solves the ODE

class AugmentedNeuralODEBlock_conv(nn.Module):
  def __init__(self,device,odefunc,tol=1e-3,max_steps=1000):
    super().__init__()
    self.odefunc = odefunc # dynamics
    self.device = device
    self.tol = tol
    self.max_steps = max_steps

  def forward(self,x,times=None):
    # x is unaugmented batched images
    if times is None:
      times = torch.tensor([0, 1]).float().type_as(x) # the times to evaluate the function at
    if self.odefunc.aug_channels>0: # augment the input
      b, c, h, w = x.shape
      aug = torch.zeros(b,self.odefunc.aug_channels,h,w).to(self.device) # augmented channels
      x = torch.cat([x,aug],dim=1)
    out = odeint_adjoint(func=self.odefunc, y0=x, t=times, rtol=self.tol, atol=self.tol, method="dopri5", \
                options={'max_num_steps': self.max_steps})
    return out[-1]

# augmented ODE network

class AugmentedNeuralODE_conv(nn.Module):
  def __init__(self,device,out_dim=1,img_size=(3,32,32),hidden_channels=64,aug_channels=5,num_layers=3,act=nn.ReLU(),tol=1e-3):
    super().__init__()
    self.device = device
    self.out_dim = out_dim
    self.img_size = img_size
    self.hidden_channels = hidden_channels
    self.aug_channels = aug_channels
    self.num_layers = num_layers
    self.act = act
    self.tol = tol
    self.flatten_dim = (img_size[0]+aug_channels) * img_size[1] * img_size[2] # flattened dimension for the linear layer

    self.odefunc = ODEFunc(device=device,img_size=img_size,hidden_channels=hidden_channels,aug_channels=aug_channels,num_layers=num_layers,act=act)
    self.augmented_neural_ode = AugmentedNeuralODEBlock_conv(device=device,odefunc=self.odefunc,tol=self.tol)
    self.linear = nn.Linear(self.flatten_dim,self.out_dim)
    self.sigmoid = nn.Sigmoid()

  def forward(self,x):
    # x is unaugmented batched images
    out = self.augmented_neural_ode(x) # phi(x), size is (b, c+aug, h, w)
    out = torch.flatten(out,start_dim=1) # flatten the image (b, (c+aug)*h*w)
    out = self.linear(out)
    out = self.sigmoid(out) # (b, 1), value between 0 and 1
    return out